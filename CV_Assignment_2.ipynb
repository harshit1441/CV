{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshit1441/CV/blob/main/CV_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXnzcKFFKs5S"
      },
      "source": [
        "# **Assignment 2**\n",
        "## **Convolutional Neural Networks (CNNS) for Image Classification**\n",
        "\n",
        "In this assignment, we explore the capabilities of a popular deep learning architectures — Convolutional Neural Networks (CNNs) — by training them on various datasets and comparing their performance. While ANNs serve as a versatile model for a range of tasks, CNNs are specifically designed for handling spatial data, making them particularly effective for image classification problems. By evaluating these models, we aim to highlight their respective strengths, limitations, and suitability for different types of data, providing insights into their real-world applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdk-N_2hKs5i"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "### **Step - 1**\n",
        "This block imports essential libraries needed for building and training a Convolutional Neural Network (CNN) with PyTorch, including data loading, transformations, and metrics for evaluation.\n",
        "1) <blue>**torch**</blue>: The core PyTorch library, essential for all operations involving tensors, model creation, and training.\n",
        "2) <blue>**torch.nn**</blue>: Contains modules to define neural network architectures.\n",
        "3) <blue>**torch.optim**</blue>: Provides optimization algorithms like <green>**SGD (Stochastic Gradient Descent)**</green>, used during model training.\n",
        "4) <blue>**torchvision.transforms**</blue>: A module for applying various image transformations, such as <green>**normalization**</green> or <green>**random cropping**</green>.\n",
        "5) <blue>**DataLoader**</blue>: Used to efficiently load data in batches, critical for training deep learning models.\n",
        "6) <blue>**sklearn.metrics**</blue>: Includes metrics to evaluate model performance, such as <green>**accuracy**</green> and <green>**precision**</green>.\n",
        "7) <blue>**seaborn/matplotlib**</blue>: Libraries for <green>**visualization**</green>, typically used to visualize model performance metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2iuKTjqKs5l"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fA3mUffKs5p"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "### **Step - 2**\n",
        "This block configures the device (CPU or GPU) and applies data transformations to the images (conversion to tensors and normalization).\n",
        "1) <blue>**torch.device**</blue>: Chooses the <green>**device**</green> to run computations on. If a <green>**GPU**</green> is available, the model will run on it; otherwise, it defaults to the CPU.\n",
        "2) <blue>**transforms.Compose**</blue>: Combines multiple transformations to apply <green>**sequentially**</green> to the data.\n",
        "3) <blue>**transforms.ToTensor()**</blue>: Converts the image from a <green>**PIL Image**</green> (or numpy array) to a PyTorch tensor.\n",
        "4) <blue>**transforms.Normalize()**</blue>: Normalizes the pixel values of the image. Each <green>**channel (R, G, B)**</green> is normalized with a <blue>**mean**</blue> of <green>**0.5**</green> and a <blue>**standard deviation**</blue> of <green>**0.5**</green>, scaling the values to the range [-1, 1]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZT-d5ht_Ks5q"
      },
      "outputs": [],
      "source": [
        "# Set device (GPU if available, else CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the dataset\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPu9JOXOKs5t"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "### **Step - 3**\n",
        "This block is meant to load the CIFAR-10 dataset, though the actual dataset loading is marked as a TODO. It sets up data loaders for both the training and testing datasets.\n",
        "1) <blue>**train_dataset/test_dataset**</blue>: These will eventually contain the training and testing datasets. The <blue>**torchvision.datasets.CIFAR10**</blue> class will likely be used here.\n",
        "2) <blue>**train_loader**</blue>: Loads the training dataset in <blue>**batches**</blue> of <green>**64**</green> images and <green>**shuffles**</green> them to ensure randomness during training.\n",
        "3) <blue>**test_loader**</blue>: Loads the test dataset in <blue>**batches**</blue> of <green>**64**</green> images but <green>**does not shuffle**</green> them since testing doesn't require randomization.\n",
        "\n",
        "Note: Replace the placeholders **\"None\"** with the appropriate code to download the training and testing dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVbR6hIyKs5w"
      },
      "outputs": [],
      "source": [
        "# Load CIFAR-10 dataset\n",
        "# TODO: Use torchvision.datasets.CIFAR10 to load the training and test sets\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD8QOcmOKs5y"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 4**\n",
        "1) This code defines a simple <green>**feedforward artificial neural network (ANN)**</green> for classification.\n",
        "2) The model has three fully connected layers <blue>**(fc1, fc2, fc3)**</blue>, and the final layer outputs predictions for <green>**10 classes** </green>.\n",
        "3) The <green>**forward pass**</green> describes how the input data flows through the network\n",
        "4) The input image is first <green>**flattened**</green>.\n",
        "5) It passes through fully connected layers with <blue>**ReLU activation function**</blue>.\n",
        "\n",
        "Note: Replace the placeholders **\"None\"** with the appropriate numbers of units and activation functions in each layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E54RPwV8Ks50"
      },
      "outputs": [],
      "source": [
        "class ANN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ANN, self).__init__()\n",
        "        # TODO: Define the layers for the ANN (Flatten, Fully Connected, Activation functions)\n",
        "        self.fc1 = nn.Linear(32*32*3, 512)  # Input layer (replace None with the number of units)\n",
        "        self.fc2 = nn.Linear(512, 256)  # Hidden layer (replace None with the number of units)\n",
        "        self.fc3 = nn.Linear(256, 10)  # Output layer for 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Define forward pass\n",
        "        x = x.view(-1, 32*32*3)  # Flatten the input image\n",
        "        x = F.relu(self.fc1(x))  # First fully connected layer + relu activation\n",
        "        x = F.relu(self.fc2(x))  # Second fully connected layer + relu activation\n",
        "        x = self.fc3(x)  # Output layer\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR6cFEx8Ks52"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 5**\n",
        "1) The <blue>**ANN model**</blue> is instantiated and moved to the <green>**selected device (CPU or GPU)**</green>.\n",
        "2) The loss function is set to <blue>**CrossEntropyLoss**</blue>, which is suitable for <green>**multi-class classification**</green> problems like in our case of CIFAR-10.\n",
        "3) The optimizer is <blue>**Adam**</blue>, with a <blue>**learning rate**</blue> of <green>**0.001**</green>, used to adjust the model parameters during training based on gradients computed from the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-g1UHuSYKs53"
      },
      "outputs": [],
      "source": [
        "# Initialize the ANN model, loss function, and optimizer\n",
        "model_ann = ANN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_ann.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdHUohymKs55"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 6**\n",
        "This code trains the neural network over a specified number of epochs (num_epochs).\n",
        "For each batch of images and labels, the following steps are performed:\n",
        "1) <blue>**Data Movement**</blue>: Images and labels are moved to the <green>**device (CPU or GPU)**</green>.\n",
        "2) <blue>**Forward Pass**</blue>: Images pass through the network to compute the <green>**output predictions**</green>.\n",
        "3) <blue>**Loss Calculation**</blue>: The loss between the predictions and true labels is computed and added to <blue>**ls_losses**</blue> for tracking.\n",
        "4) <blue>**Backpropagation and Optimization**</blue>: Gradients are calculated using backpropagation, and the optimizer <green>**updates the model parameters**</green> based on these gradients.\n",
        "\n",
        "Every 100 batches, the loss is printed to monitor training progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "QOvLNvZBKs56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "b219898d-8823-4ee4-d0b0-3cc7bd784bb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [100/782], Loss: 1.8182\n",
            "Epoch [1/5], Step [200/782], Loss: 1.6449\n",
            "Epoch [1/5], Step [300/782], Loss: 1.6415\n",
            "Epoch [1/5], Step [400/782], Loss: 1.5004\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3527064439.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mls_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m# TODO: Move images and labels to the device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace None with the correct code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "num_epochs = 5\n",
        "ls_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # TODO: Move images and labels to the device\n",
        "        images = images.to(device)  # Replace None with the correct code\n",
        "        labels = labels.to(device)\n",
        "\n",
        "\n",
        "        # TODO: Forward pass\n",
        "        outputs = model_ann(images)  # Replace None with forward pass through the model\n",
        "        loss = criterion(outputs, labels) # Replace None with the correct code to find error between labels and outputs\n",
        "        ls_losses.append(loss.detach().numpy())\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oGJSpn5Ks57"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 7**\n",
        "1) This code plots the <blue>**training losses**</blue> that were recorded in <blue>**ls_losses**</blue> during training.\n",
        "2) The <blue>**x-axis**</blue> represents the <green>**number of samples (batches)**</green> seen during training, and the <blue>**y-axis**</blue> shows the <green>**corresponding loss**</green>.\n",
        "3) It provides a visual representation of how the <green>**model's loss decreases over time**</green>, indicating whether the training is progressing well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsCwnhvtKs58"
      },
      "outputs": [],
      "source": [
        "# Plot Losses\n",
        "x_axis = np.arange(0, len(ls_losses), 1)\n",
        "plt.plot(x_axis, ls_losses)\n",
        "plt.xlabel = \"Sample\"\n",
        "plt.ylabel = \"Loss\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A20nOS-6Ks58"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 8**\n",
        "This block evaluates the model on the test dataset.\\\n",
        "<blue>**model.eval()**</blue> sets the model to <green>**evaluation mode**</green>, disabling dropout layers and stopping the computation of gradients to save memory and speed up computations.\n",
        "For each batch of test images:\n",
        "1) <blue>**Data Movement**</blue>: Images and labels are moved to the <green>**device (CPU/GPU)**</green>.\n",
        "2) <blue>**Forward Pass**</blue>: Images pass through the network to compute <green>**predictions**</green>.\n",
        "3) <blue>**Storing Results**</blue>: Predictions and true labels are saved to <green>**calculate metrics**</green> later.\n",
        "\n",
        "This code also calculates key performance metrics to evaluate the model:\n",
        "1) <blue>**Accuracy**</blue>: Percentage of correctly classified samples.\n",
        "2) <blue>**Precision**</blue>: Proportion of true positive predictions out of all positive predictions.\n",
        "3) <blue>**Recall**</blue>: Proportion of true positives out of actual positive samples.\n",
        "4) <blue>**F1-Score**</blue>: <green>**Harmonic mean**</green> of precision and recall.\n",
        "5) The <blue>**confusion matrix**</blue> is also calculated, showing the number of correct and incorrect predictions for each class. It is visualized using a <green>**heatmap**</green>, where the <blue>**rows**</blue> represent <green>**true labels**</green>, and the <blue>**columns**</blue> represent <green>**predicted labels**</green>.\n",
        "\n",
        "Note: Replace the placeholders **\"None\"** with the appropriate code to calculate the metrics using sklearn and compute the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nFG5UohKs5_"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "model_ann.eval()\n",
        "all_preds_ann = []\n",
        "all_labels_ann = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # TODO: Move images and labels to the device\n",
        "        images = images.to(device)  # Replace None with the correct code\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # TODO: Forward pass\n",
        "        outputs = model_ann(images)  # Replace None with the forward pass\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        all_preds_ann.extend(predicted.cpu().numpy())\n",
        "        all_labels_ann.extend(labels.cpu().numpy())\n",
        "\n",
        "# TODO: Calculate accuracy, precision, recall, and F1-score using sklearn\n",
        "accuracy_ann = accuracy_score(all_labels_ann, all_preds_ann)\n",
        "precision_ann = precision_score(all_labels_ann, all_preds_ann, average='macro')\n",
        "recall_ann = recall_score(all_labels_ann, all_preds_ann, average='macro')\n",
        "f1_ann = f1_score(all_labels_ann, all_preds_ann, average='macro')\n",
        "\n",
        "print(f\"ANN Accuracy: {accuracy_ann:.4f}\")\n",
        "print(f\"ANN Precision: {precision_ann:.4f}\")\n",
        "print(f\"ANN Recall: {recall_ann:.4f}\")\n",
        "print(f\"ANN F1-Score: {f1_ann:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le6yLSF6Ks6A"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "### **Step - 9**\n",
        "This block defines a simple Convolutional Neural Network (CNN) architecture using PyTorch. The model will consist of convolutional layers, activation functions, pooling layers, and fully connected layers to classify images from the CIFAR-10 dataset.\n",
        "1) <blue>**class CNN(nn.Module)**</blue>: This defines a custom CNN class that inherits from PyTorch's nn.Module, the base class for all neural networks in PyTorch.\n",
        "2) <blue>**self.conv**</blue>: These define the two convolutional layers. The first takes an <blue>**input**</blue> with <green>**3 channels (RGB)**</green> and produces <green>**32**</green> <blue>**feature maps**</blue>. The second takes <green>**32**</green> <blue>**input channels**</blue> and produces <green>**64**</green> <blue>**feature maps**</blue>.\n",
        "3) <blue>**self.pool**</blue>: A max-pooling layer that reduces the size of the feature maps by <green>**half (downsampling)**</green>. It takes the maximum value over a <green>**2x2**</green> <blue>**grid**</blue> with a <blue>**stride**</blue> of <green>**2**</green>.\n",
        "4) <blue>**self.fc**</blue>: Fully connected layers. The first layer takes the flattened feature maps from the convolutional layers as input and <blue>**outputs**</blue> <green>**512**</green> features. The second layer maps the <green>**512**</green> <blue>**features**</blue> to <green>**10**</green> <blue>**output classes**</blue> (for the 10 CIFAR-10 categories).\n",
        "5) <blue>**forward(self, x)**</blue>: This function defines how data flows through the network. It applies the convolutional layers, pooling, and fully connected layers in sequence.\n",
        "\n",
        "Note: Replace the placeholders **\"None\"** with the appropriate code to calculate the metrics using sklearn and compute the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcQdhK8bKs6C"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        # TODO: Define the CNN layers (Conv2D, MaxPool, Fully Connected)\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # Replace None with the correct number of filters\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # Replace None\n",
        "        self.fc1 = nn.Linear(64*8*8, 256)  # Replace None\n",
        "        self.fc2 = nn.Linear(256, 10)  # Output layer for 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # First conv layer + relu activation + pool\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # Second conv layer + relu activation + pool\n",
        "        x = x.view(-1, 64 * 8 * 8)  # Flatten the tensor (replace None with correct dimension)\n",
        "        x = F.relu(self.fc1(x))  # Fully connected layer + relu activation\n",
        "        x = self.fc2(x)  # Output layer\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7X2ZLwpHKs6D"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "### **Step - 10**\n",
        "This block defines the training loop for the CNN model. It will train the model over multiple epochs, compute the loss using cross-entropy, and optimize the model using an optimizer like SGD.\n",
        "1) <blue>**optimizer.zero_grad()**</blue>: <green>**Clears**</green> the <blue>**gradients**</blue> from the previous iteration to prevent accumulation.\n",
        "2) <blue>**loss.backward()**</blue>: Computes the <blue>**gradients**</blue> via <green>**backpropagation**</green>.\n",
        "3) <blue>**optimizer.step()**</blue>: Updates the model's parameters based on the <green>**computed gradients**</green>.\n",
        "4) <blue>**running_loss**</blue>: Keeps track of the <blue>**cumulative loss**</blue> for the epoch, which is divided by the <green>**number of batches**</green> to return the <blue>**average loss**</blue>.\n",
        "\n",
        "Note: Replace the placeholders **\"None\"** with the appropriate code to calculate the metrics using sklearn and compute the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSZcYh_CKs6F"
      },
      "outputs": [],
      "source": [
        "# Initialize the CNN model, loss function, and optimizer\n",
        "model_cnn = CNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_cnn.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # TODO: Move images and labels to the device\n",
        "        images = images.to(device)  # Replace None\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # TODO: Forward pass\n",
        "        outputs = model_cnn(images)  # Replace None with forward pass through the CNN\n",
        "        loss = criterion(outputs, labels) # Replace None with the correct code to find error between labels and outputs\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXgekN2Ks6G"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "### **Step - 11**\n",
        "This code performs evaluation on the test set by moving data to the appropriate device, running the model to get predictions, and then calculating key performance metrics using the predicted and actual labels.\n",
        "1) <blue>**model_cnn.eval()**</blue>: The code begins by setting the model to <green>**evaluation mode**</green> to ensure proper inference behavior.\n",
        "2) <blue>**all_preds_cnn, all_labels_cnn**</blue>: These lists are initialized to store <green>**predicted**</green> and <green>**true labels**</green>, respectively.\n",
        "3) <blue>**torch.no_grad()**</blue>: This is used to <green>**disable gradients**</green>, save memory and speed up computations, the test set is processed in batches from test_loader, where both images and labels are moved to the correct device.\n",
        "6) After processing the test set, evaluation metrics such as <blue>**accuracy**</blue>, <blue>**precision**</blue>, <blue>**recall**</blue>, and <blue>**F1-score**</blue> are calculated.\n",
        "7) Finally, the results are printed to display the performance of the CNN model\n",
        "\n",
        "Note: Replace the placeholders **\"None\"** with the appropriate code to calculate the metrics using sklearn and compute the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HkMOfQbKs6J"
      },
      "outputs": [],
      "source": [
        "# Test the CNN model\n",
        "model_cnn.eval()\n",
        "all_preds_cnn = []\n",
        "all_labels_cnn = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # TODO: Move images and labels to the device\n",
        "        images = images.to(device)  # Replace None\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model_cnn(images)  # Replace None with forward pass through the CNN\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        all_preds_cnn.extend(predicted.cpu().numpy())\n",
        "        all_labels_cnn.extend(labels.cpu().numpy())\n",
        "\n",
        "# TODO: Calculate accuracy, precision, recall, and F1-score for the CNN\n",
        "accuracy_cnn = accuracy_score(all_labels_cnn, all_preds_cnn)\n",
        "precision_cnn = precision_score(all_labels_cnn, all_preds_cnn, average='macro')\n",
        "recall_cnn = recall_score(all_labels_cnn, all_preds_cnn, average='macro')\n",
        "f1_cnn = f1_score(all_labels_cnn, all_preds_cnn, average='macro')\n",
        "\n",
        "print(f\"CNN Accuracy: {accuracy_cnn:.4f}\")\n",
        "print(f\"CNN Precision: {precision_cnn:.4f}\")\n",
        "print(f\"CNN Recall: {recall_cnn:.4f}\")\n",
        "print(f\"CNN F1-Score: {f1_cnn:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Otr_qj-TKs6M"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "### **Step - 12**\n",
        "This code block compares performance of the trained ANN and CNN models, showing which one performs better.\n",
        "1) <blue>**import pandas as pd**</blue>: The code starts by importing the <green>**Pandas library**</green>, which is used for data manipulation and creation of a comparison table.\n",
        "2) <blue>**data dictionary**</blue>: A dictionary is defined with the keys 'Model', 'Accuracy', 'Precision', 'Recall', and 'F1-Score', containing <green>**placeholders (None)**</green> for both the 'ANN' and 'CNN' models. These placeholders will later hold the actual performance metrics.\n",
        "3) <blue>**pd.DataFrame(data)**</blue>: The dictionary is converted into a <green>**Pandas DataFrame**</green>, which provides a tabular structure for easy comparison of the metrics between the two models.\n",
        "4) <blue>**print(df_comparison)**</blue>: This line prints the <green>**comparison table**</green> of ANN and CNN metrics, enabling a visual representation of their performance.\n",
        "\n",
        "Note: Replace the placeholders **\"None\"** with the appropriate code to calculate the metrics using sklearn and compute the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aF5qJjUcKs6N"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# TODO: Create a comparison table\n",
        "data = {\n",
        "    'Model': ['ANN', 'CNN'],\n",
        "    'Accuracy': [accuracy_ann, accuracy_cnn],      # Replace None with the accuracy of ANN and CNN\n",
        "    'Precision': [precision_ann, precision_cnn],\n",
        "    'Recall': [recall_ann, recall_cnn],\n",
        "    'F1-Score': [f1_ann, f1_cnn]\n",
        "}\n",
        "\n",
        "df_comparison = pd.DataFrame(data)\n",
        "print(df_comparison)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advanced Steps (Optional for people targeting basic level)"
      ],
      "metadata": {
        "id": "IAW_O7npNUjU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b504987"
      },
      "source": [
        "### **Step - 13 (Understanding CNN Filters)**\n",
        "This step aims to provide insight into what the convolutional filters in the CNN model have learned. By visualizing the weights of the filters in the first convolutional layer, we can get a sense of the basic features (like edges, corners, or textures) that the network is detecting in the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d21965d"
      },
      "source": [
        "# Visualize filters from the first convolutional layer of the CNN model\n",
        "\n",
        "# TODO: Get the weights of the first convolutional layer\n",
        "first_conv_layer_weights = model_cnn.conv1.weight.data.cpu().numpy()  # Replace None with code to extract layer weights (e.g., model_cnn.conv1.weight.data.cpu().numpy())\n",
        "\n",
        "# TODO: Normalize the weights for visualization\n",
        "# Normalizing helps bring all filter values to a common scale between 0 and 1\n",
        "min_w = first_conv_layer_weights.min()  # Replace None with code to find the minimum value in the weights\n",
        "max_w = first_conv_layer_weights.max()  # Replace None with code to find the maximum value in the weights\n",
        "first_conv_layer_weights = (first_conv_layer_weights - min_w) / (max_w - min_w)   # Replace None with normalization formula\n",
        "\n",
        "# TODO: Plot the filters\n",
        "# We’ll display up to 16 filters for clarity\n",
        "num_filters = first_conv_layer_weights.shape[0]  # Replace None with the total number of filters in the first layer\n",
        "num_plots = min(num_filters, 16)\n",
        "fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(10, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i in range(num_plots):\n",
        "    # TODO: Select one filter to visualize\n",
        "    filter_img = np.transpose(first_conv_layer_weights[i], (1, 2, 0))  # Replace None with correct slice from first_conv_layer_weights\n",
        "    axes[i].imshow(filter_img, cmap='gray')\n",
        "    axes[i].set_title(f'Filter {i+1}')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "# Hide unused subplots\n",
        "for j in range(num_plots, len(axes)):\n",
        "    axes[j].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62b0e7ca"
      },
      "source": [
        "### **Step - 14 (Data Augmentation)**\n",
        "Data augmentation is a technique used to increase the diversity of the training dataset by applying random transformations such as rotations, flips, and crops to the images. This helps to prevent overfitting and can improve the generalization ability of the model. We will apply data augmentation to the training data and retrain the CNN model to see if it improves performance."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data augmentation transformations\n",
        "train_transform_augmented = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset with data augmentation\n",
        "train_dataset_augmented = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                                      download=True, transform=train_transform_augmented)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform) # Use the original transform for testing\n",
        "\n",
        "train_loader_augmented = DataLoader(dataset=train_dataset_augmented, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# TODO: Initialize CNN model, loss function, and optimizer\n",
        "model_cnn_augmented = CNN().to(device)             # Replace None with model initialization\n",
        "criterion = nn.CrossEntropyLoss()                  # Replace None with loss function\n",
        "optimizer_augmented = optim.Adam(model_cnn_augmented.parameters(), lr=0.001)  # Replace None with optimizer\n",
        "\n",
        "print(\"Training CNN with Data Augmentation...\")\n",
        "\n",
        "# TODO: Train the CNN model\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader_augmented):\n",
        "\n",
        "        # Move images and labels to the device\n",
        "        images = images.to(device)  # Replace None with code to move images to device\n",
        "        labels = labels.to(device)  # Replace None with code to move labels to device\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_cnn_augmented(images)  # Replace None with model forward pass\n",
        "        loss = criterion(outputs, labels)      # Replace None with loss computation\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer_augmented.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_augmented.step()\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader_augmented)}], Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "MU-uJXJVO7hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6d1dfb5"
      },
      "source": [
        "### **Step - 15 (Visualize Incorrect Predictions)**\n",
        "Visualizing some of the images that the CNN model misclassified can provide valuable insights into the model's weaknesses and help identify areas for improvement. This step will display a few examples of images where the model's predicted label does not match the true label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcf50c4c"
      },
      "source": [
        "# Visualize some incorrect predictions from the CNN model (using the augmented model if trained)\n",
        "# TODO: Choose which model to evaluate (use augmented model if available)\n",
        "model_to_evaluate = model_cnn_augmented if 'model_cnn_augmented' in globals() else model_cnn  # Replace None with conditional model selection\n",
        "model_to_evaluate.eval()\n",
        "\n",
        "\n",
        "incorrect_preds = []\n",
        "incorrect_labels = []\n",
        "incorrect_images = []\n",
        "\n",
        "# TODO: Run the model on the test dataset\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images_cpu = images.cpu()          # Replace None with a CPU copy of images for visualization\n",
        "        images = images.to(device)         # Replace None with code to move images to device\n",
        "        labels = labels.to(device)         # Replace None with code to move labels to device\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_to_evaluate(images)                    # Replace None with model forward pass\n",
        "        _, predicted = torch.max(outputs.data, 1)             # Replace None with code to get class predictions using torch.max()\n",
        "\n",
        "        for i in range(len(labels)):\n",
        "            if predicted[i] != labels[i]:\n",
        "                incorrect_preds.append(predicted[i].item())\n",
        "                incorrect_labels.append(labels[i].item())\n",
        "                # Denormalize the image before storing\n",
        "                img = images_cpu[i].numpy().transpose((1, 2, 0))\n",
        "                img = 0.5 * img + 0.5 # Denormalize\n",
        "                img = np.clip(img, 0, 1)\n",
        "                incorrect_images.append(img)\n",
        "\n",
        "# Display up to 10 incorrect predictions, add more visualisations if possible\n",
        "num_to_display = min(len(incorrect_images), 10)\n",
        "fig, axes = plt.subplots(nrows=1, ncols=num_to_display, figsize=(20, 4))\n",
        "\n",
        "if num_to_display > 0:\n",
        "    for i in range(num_to_display):\n",
        "        axes[i].imshow(incorrect_images[i])\n",
        "        axes[i].set_title(f'Pred: {incorrect_preds[i]}, True: {incorrect_labels[i]}')\n",
        "        axes[i].axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No incorrect predictions to display.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WkD3QwE2dO6K"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}