{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshit1441/CV/blob/main/CV_final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykndg5U_xRlh",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Install the Segment Anything Model (SAM) from Meta\n",
        "!pip install segment-anything\n",
        "#  Install PyTorch-based segmentation and detection libraries\n",
        "# YOLOv8 is a modern, easy-to-use option for detection and segmentation\n",
        "!pip install ultralytics\n",
        "#  Install pycocotools for processing the COCO dataset (if you choose it)\n",
        "!pip install pycocotools\n",
        "#  Install general utility libraries (if not already installed)\n",
        "!pip install opencv-python Pillow scipy scikit-image\n",
        "\n",
        "# Deep Learning Framework (PyTorch)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Vision/Image Processing\n",
        "import numpy as np\n",
        "import cv2 # OpenCV for image manipulation\n",
        "from PIL import Image # Python Imaging Library\n",
        "import matplotlib.pyplot as plt # For visualization\n",
        "\n",
        "# Data Utilities\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm # Progress bar\n",
        "\n",
        "# Foundation Models (SAM/Segment Anything)\n",
        "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry # Official SAM library\n",
        "\n",
        "# Traditional/Modern CV Models (U-Net, YOLO)\n",
        "# For U-Net, you would typically define the architecture (or use a library like 'segmentation_models.pytorch')\n",
        "# For YOLOv8, you can import the model directly:\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# COCO Dataset Utilities (if using COCO 2017)\n",
        "# Note: You might need to install pycocotools from the official GitHub if the pip version fails\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.mask import decode, toBbox"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU VERIFICATION CELL\n",
        "\n",
        "# Check Colab's allocated GPU (should show 'Tesla T4' or similar)\n",
        "!nvidia-smi\n",
        "\n",
        "# Check if PyTorch can detect a CUDA-enabled GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "NDkGIoFLyHG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DOWNLOAD COCO 2017 VALIDATION IMAGES\n",
        "# This file contains 5,000 validation images\n",
        "print(\"Downloading COCO val2017 images (815MB)...\")\n",
        "!wget -q http://images.cocodataset.org/zips/val2017.zip\n",
        "print(\"Unzipping images...\")\n",
        "!unzip -q val2017.zip\n",
        "print(\"Image download complete. Folder 'val2017' created.\")\n",
        "\n",
        "# Clean up the zip file to save space\n",
        "!rm val2017.zip"
      ],
      "metadata": {
        "id": "uMfVyJ7OyZe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DOWNLOAD COCO 2017 ANNOTATIONS\n",
        "# This file contains the .json files describing the dataset\n",
        "print(\"Downloading COCO annotations (241MB)...\")\n",
        "!wget -q http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "print(\"Unzipping annotations...\")\n",
        "!unzip -q annotations_trainval2017.zip\n",
        "print(\"Annotations download complete. Folder 'annotations' created.\")\n",
        "\n",
        "# Clean up the zip file\n",
        "!rm annotations_trainval2017.zip"
      ],
      "metadata": {
        "id": "x9AroI2BybXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD AND VERIFY ANNOTATIONS ---\n",
        "from pycocotools.coco import COCO\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Define paths\n",
        "dataDir = '.'\n",
        "dataType = 'val2017'\n",
        "annFile = os.path.join(dataDir, 'annotations', f'instances_{dataType}.json')\n",
        "imgDir = os.path.join(dataDir, dataType)\n",
        "\n",
        "# Initialize COCO api for instance annotations\n",
        "print(f\"Loading annotations from: {annFile}\")\n",
        "coco = COCO(annFile)\n",
        "\n",
        "# Get all image IDs\n",
        "imgIds = coco.getImgIds()\n",
        "print(f\"Successfully loaded {len(imgIds)} image annotations.\")\n",
        "\n",
        "# --- Define our subset ---\n",
        "# We will just use the first 1000 images to meet the project spec\n",
        "subset_img_ids = imgIds[:1000]\n",
        "print(f\"We will use a subset of {len(subset_img_ids)} images.\")\n",
        "\n",
        "# --- Test: Load and display a random image from our subset ---\n",
        "random_img_id = random.choice(subset_img_ids)\n",
        "img_info = coco.loadImgs([random_img_id])[0]\n",
        "\n",
        "# Load image\n",
        "img_path = os.path.join(imgDir, img_info['file_name'])\n",
        "image = cv2.imread(img_path)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Load and display annotations\n",
        "annIds = coco.getAnnIds(imgIds=img_info['id'])\n",
        "anns = coco.loadAnns(annIds)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "coco.showAnns(anns, draw_bbox=True)\n",
        "plt.title(f\"Test: Image ID {random_img_id} with masks\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1txZ-dcKzL6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DEFINE THE CUSTOM COCO DATASET ---\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "\n",
        "class CocoSegmentationDataset(Dataset):\n",
        "    def __init__(self, img_dir, coco_api, img_ids, transforms=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.coco = coco_api\n",
        "        self.img_ids = img_ids\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the size of our subset\n",
        "        return len(self.img_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 1. Get image info\n",
        "        img_id = self.img_ids[idx]\n",
        "        img_info = self.coco.loadImgs([img_id])[0]\n",
        "        img_path = os.path.join(self.img_dir, img_info['file_name'])\n",
        "\n",
        "        # 2. Load image\n",
        "        # Open with PIL for easier transform compatibility\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # 3. Load all annotations for this image\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=img_info['id'])\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        # 4. Create a single combined binary mask\n",
        "        # Start with an empty mask (all background)\n",
        "        mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)\n",
        "\n",
        "        # Iterate over all annotations and \"draw\" them onto the mask\n",
        "        for ann in anns:\n",
        "            # coco.annToMask returns a binary mask for ONE annotation\n",
        "            mask = np.maximum(mask, self.coco.annToMask(ann))\n",
        "\n",
        "        # Convert numpy mask to PIL Image (so transforms can be applied)\n",
        "        mask = Image.fromarray(mask)\n",
        "\n",
        "        # 5. Apply transforms (if any)\n",
        "        if self.transforms is not None:\n",
        "            image, mask = self.transforms(image, mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "print(\"Custom Dataset class 'CocoSegmentationDataset' defined.\")"
      ],
      "metadata": {
        "id": "yZQXbdaPzgt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  DEFINE TRANSFORMS AND CREATE DATALOADER ---\n",
        "\n",
        "# Define the image transformations\n",
        "# We need to apply the same geometric transforms (like Resize) to BOTH\n",
        "# the image and the mask, but normalization ONLY to the image.\n",
        "\n",
        "IMG_SIZE = 256\n",
        "\n",
        "# Define transforms\n",
        "def get_transforms():\n",
        "    class ResizeAndToTensor:\n",
        "        def __call__(self, image, mask):\n",
        "            # 1. Resize\n",
        "            image = T.functional.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "            # Use NEAREST neighbor for masks to keep 0/1 values sharp\n",
        "            mask = T.functional.resize(mask, (IMG_SIZE, IMG_SIZE), interpolation=T.InterpolationMode.NEAREST)\n",
        "\n",
        "            # 2. Convert Image to Tensor (and scale)\n",
        "            image = T.functional.to_tensor(image)\n",
        "\n",
        "            # 3. Convert Mask to Tensor (MANUALLY to avoid scaling)\n",
        "            # Convert PIL mask to numpy array (H, W)\n",
        "            mask_np = np.array(mask)\n",
        "            # Add a channel dimension (H, W) -> (1, H, W)\n",
        "            mask_np = np.expand_dims(mask_np, axis=0)\n",
        "            # Convert numpy array to tensor (values 0 & 1 are preserved)\n",
        "            # and cast to float, as required by the loss function\n",
        "            mask = torch.from_numpy(mask_np).float()\n",
        "\n",
        "            # 4. Normalize Image\n",
        "            image = T.functional.normalize(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "            return image, mask\n",
        "\n",
        "    return ResizeAndToTensor()\n",
        "\n",
        "# --- Create the Dataset and DataLoader ---\n",
        "\n",
        "# Use the 'subset_img_ids' (our 1000 images) from Cell 5\n",
        "train_dataset = CocoSegmentationDataset(\n",
        "    img_dir=imgDir,\n",
        "    coco_api=coco,\n",
        "    img_ids=subset_img_ids,\n",
        "    transforms=get_transforms()\n",
        ")\n",
        "\n",
        "# Create the DataLoader\n",
        "# This will feed data to our model in batches of 8\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
        "\n",
        "print(f\"Created a training dataset with {len(train_dataset)} images.\")\n",
        "print(\"Created 'train_loader' to feed batches of 8 images/masks to the model.\")\n",
        "\n",
        "# --- Test the DataLoader ---\n",
        "# Let's grab one batch and check its shape\n",
        "try:\n",
        "    images, masks = next(iter(train_loader))\n",
        "    print(f\"\\nSuccessfully loaded one batch:\")\n",
        "    print(f\"Images batch shape: {images.shape}\") # [BatchSize, Channels, Height, Width]\n",
        "    print(f\"Masks batch shape: {masks.shape}\")   # [BatchSize, 1, Height, Width]\n",
        "    print(f\"Masks data type: {masks.dtype}\")\n",
        "    print(f\"Mask min/max: {masks.min()}, {masks.max()}\") # Should be 0.0 and 1.0\n",
        "except Exception as e:\n",
        "    print(f\"\\nError loading batch: {e}\")\n",
        "    print(\"This can sometimes happen on the first run, try re-running this cell.\")"
      ],
      "metadata": {
        "id": "zmO3KRRZ0k3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DEFINE THE U-NET ARCHITECTURE ---\n",
        "\n",
        "# First, define the building block: a double convolution layer\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "# Define the U-Net Model\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        # Encoder (Down-path)\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = (DoubleConv(64, 128), nn.MaxPool2d(2))\n",
        "        self.down2 = (DoubleConv(128, 256), nn.MaxPool2d(2))\n",
        "        self.down3 = (DoubleConv(256, 512), nn.MaxPool2d(2))\n",
        "        self.down4 = (DoubleConv(512, 1024), nn.MaxPool2d(2))\n",
        "\n",
        "        # Decoder (Up-path)\n",
        "        # Note: Up-conv + skip connection concatenation\n",
        "        self.up1 = (nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2), DoubleConv(1024, 512))\n",
        "        self.up2 = (nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2), DoubleConv(512, 256))\n",
        "        self.up3 = (nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2), DoubleConv(256, 128))\n",
        "        self.up4 = (nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2), DoubleConv(128, 64))\n",
        "\n",
        "        # Final output convolution\n",
        "        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n",
        "\n",
        "        # Register modules to make them accessible\n",
        "        self.d1 = nn.Sequential(*self.down1)\n",
        "        self.d2 = nn.Sequential(*self.down2)\n",
        "        self.d3 = nn.Sequential(*self.down3)\n",
        "        self.d4 = nn.Sequential(*self.down4)\n",
        "        self.u1 = nn.ModuleList(self.up1)\n",
        "        self.u2 = nn.ModuleList(self.up2)\n",
        "        self.u3 = nn.ModuleList(self.up3)\n",
        "        self.u4 = nn.ModuleList(self.up4)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.d1(x1)\n",
        "        x3 = self.d2(x2)\n",
        "        x4 = self.d3(x3)\n",
        "        x5 = self.d4(x4) # Bottleneck\n",
        "\n",
        "        # Decoder + Skip Connections\n",
        "        x = self.u1[0](x5) # Up-conv\n",
        "        x = torch.cat([x, x4], dim=1) # Skip connection\n",
        "        x = self.u1[1](x) # DoubleConv\n",
        "\n",
        "        x = self.u2[0](x)\n",
        "        x = torch.cat([x, x3], dim=1)\n",
        "        x = self.u2[1](x)\n",
        "\n",
        "        x = self.u3[0](x)\n",
        "        x = torch.cat([x, x2], dim=1)\n",
        "        x = self.u3[1](x)\n",
        "\n",
        "        x = self.u4[0](x)\n",
        "        x = torch.cat([x, x1], dim=1)\n",
        "        x = self.u4[1](x)\n",
        "\n",
        "        # Final output\n",
        "        logits = self.outc(x)\n",
        "        return logits\n",
        "\n",
        "print(\"U-Net model architecture defined.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yX5mGDjE0655"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELL 9 (Corrected): INITIALIZE MODEL AND LOSSES ---\n",
        "\n",
        "# 1. Initialize a NEW model (to restart training)\n",
        "model = UNet(n_channels=3, n_classes=1).to(device)\n",
        "\n",
        "# 2. Define BCE Loss (as before)\n",
        "bce_criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# 3. Define Dice Loss (better for imbalance)\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, smooth=1e-6):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, preds, targets):\n",
        "        # Apply sigmoid to model logits\n",
        "        preds = torch.sigmoid(preds)\n",
        "\n",
        "        # Flatten tensors\n",
        "        preds_flat = preds.view(-1)\n",
        "        targets_flat = targets.view(-1)\n",
        "\n",
        "        # Calculate intersection and union\n",
        "        intersection = (preds_flat * targets_flat).sum()\n",
        "        total = (preds_flat + targets_flat).sum()\n",
        "\n",
        "        # Calculate Dice coefficient\n",
        "        dice_coeff = (2. * intersection + self.smooth) / (total + self.smooth)\n",
        "\n",
        "        # Return Dice Loss\n",
        "        return 1 - dice_coeff\n",
        "\n",
        "dice_criterion = DiceLoss().to(device)\n",
        "\n",
        "# 4. Define the Optimizer (as before)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "print(f\"Model re-initialized on {device}.\")\n",
        "print(\"Using COMBINED loss: BCEWithLogitsLoss + DiceLoss\")"
      ],
      "metadata": {
        "id": "w9IXTnHa1AqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DEFINE METRIC FUNCTION (IoU) ---\n",
        "\n",
        "def calculate_iou(preds, targets, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Calculates the Intersection over Union (IoU) metric, also known as\n",
        "    the Jaccard index. For binary segmentation, this is often used.\n",
        "\n",
        "    preds: Model outputs (logits)\n",
        "    targets: Ground truth masks (0.0 or 1.0)\n",
        "    \"\"\"\n",
        "    # 1. Apply sigmoid to logits and threshold to get binary predictions\n",
        "    preds_binary = (torch.sigmoid(preds) > threshold).float()\n",
        "\n",
        "    # 2. Flatten tensors\n",
        "    preds_flat = preds_binary.view(-1)\n",
        "    targets_flat = targets.view(-1)\n",
        "\n",
        "    # 3. Calculate Intersection and Union\n",
        "    intersection = (preds_flat * targets_flat).sum()\n",
        "    total = (preds_flat + targets_flat).sum()\n",
        "    union = total - intersection\n",
        "\n",
        "    # 4. Calculate IoU (add 1e-6 to avoid division by zero)\n",
        "    iou = (intersection + 1e-6) / (union + 1e-6)\n",
        "\n",
        "    return iou\n",
        "\n",
        "print(\"Helper function 'calculate_iou' defined.\")"
      ],
      "metadata": {
        "id": "edXW6uPF1RZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELL 11 (Corrected): TRAINING LOOP WITH COMBINED LOSS ---\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "NUM_EPOCHS = 20 # Increased epochs\n",
        "\n",
        "print(f\"--- Starting Training on {device} for {NUM_EPOCHS} Epochs ---\")\n",
        "\n",
        "train_loss_history = []\n",
        "train_iou_history = []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_iou = 0.0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "\n",
        "    for i, (images, masks) in enumerate(progress_bar):\n",
        "\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # 2. Calculate COMBINED loss\n",
        "        bce_loss = bce_criterion(outputs, masks)\n",
        "        dice_loss = dice_criterion(outputs, masks)\n",
        "        loss = bce_loss + dice_loss # <--- THE FIX!\n",
        "\n",
        "        # 3. Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # 4. Update running metrics\n",
        "        running_loss += loss.item()\n",
        "        running_iou += calculate_iou(outputs.detach(), masks.detach()).item()\n",
        "\n",
        "        progress_bar.set_postfix({'loss': running_loss / (i + 1), 'iou': running_iou / (i + 1)})\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_iou = running_iou / len(train_loader)\n",
        "\n",
        "    train_loss_history.append(epoch_loss)\n",
        "    train_iou_history.append(epoch_iou)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {epoch_loss:.4f}, IoU: {epoch_iou:.4f}\")\n",
        "\n",
        "print(\"--- Training Complete ---\")"
      ],
      "metadata": {
        "id": "pq2lQHFG1Vgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELL 12: VISUALIZE U-NET PREDICTIONS ---\n",
        "\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "# 1. Set model to evaluation mode (disables dropout, batchnorm updates, etc.)\n",
        "model.eval()\n",
        "\n",
        "# 2. Get a test batch from the dataloader\n",
        "with torch.no_grad(): # Disable gradient calculation for inference\n",
        "    # Let's get a batch\n",
        "    images, masks = next(iter(train_loader))\n",
        "\n",
        "    # Pick one image from the batch (e.g., the first one)\n",
        "    test_image = images[0:1].to(device) # Keep batch dim [1, C, H, W]\n",
        "    true_mask = masks[0:1].to(device) # Keep batch dim [1, 1, H, W]\n",
        "\n",
        "    # 3. Get model prediction (output is logits)\n",
        "    pred_logits = model(test_image)\n",
        "\n",
        "    # 4. Process the prediction\n",
        "    # Apply sigmoid to get probabilities (0.0 to 1.0)\n",
        "    pred_probs = torch.sigmoid(pred_logits)\n",
        "    # Apply a threshold to get a binary mask (0.0 or 1.0)\n",
        "    pred_mask = (pred_probs > 0.5).float()\n",
        "\n",
        "    # 5. Move tensors to CPU and convert to NumPy for plotting\n",
        "    test_image_cpu = test_image.cpu().squeeze(0) # Remove batch dim\n",
        "    true_mask_cpu = true_mask.cpu().squeeze(0)   # Remove batch dim\n",
        "    pred_mask_cpu = pred_mask.cpu().squeeze(0)   # Remove batch dim\n",
        "\n",
        "    # --- De-normalize the image for correct display ---\n",
        "    # We need to reverse the normalization applied in the dataloader\n",
        "    # Mean = [0.485, 0.456, 0.406], Std = [0.229, 0.224, 0.225]\n",
        "    inv_normalize = T.Normalize(\n",
        "        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "        std=[1/0.229, 1/0.224, 1/0.225]\n",
        "    )\n",
        "    img_display = inv_normalize(test_image_cpu)\n",
        "\n",
        "    # Convert from tensor [C, H, W] to numpy [H, W, C] for plt.imshow\n",
        "    img_display = img_display.permute(1, 2, 0).numpy()\n",
        "    true_mask_display = true_mask_cpu.permute(1, 2, 0).numpy()\n",
        "    pred_mask_display = pred_mask_cpu.permute(1, 2, 0).numpy()\n",
        "\n",
        "    # Clip values to [0, 1] just in case of numerical instability\n",
        "    img_display = np.clip(img_display, 0, 1)\n",
        "\n",
        "    # 6. Plot the results\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    ax[0].imshow(img_display)\n",
        "    ax[0].set_title(\"Original Image\")\n",
        "    ax[0].axis('off')\n",
        "\n",
        "    ax[1].imshow(true_mask_display, cmap='gray')\n",
        "    ax[1].set_title(\"Ground Truth Mask\")\n",
        "    ax[1].axis('off')\n",
        "\n",
        "    ax[2].imshow(pred_mask_display, cmap='gray')\n",
        "    ax[2].set_title(\"U-Net Predicted Mask\")\n",
        "    ax[2].axis('off')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "3k5zqDfK5lWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVE THE TRAINED MODEL WEIGHTS ---\n",
        "\n",
        "# Define a path to save the model\n",
        "MODEL_SAVE_PATH = \"unet_coco_subset.pth\"\n",
        "\n",
        "# Save the model's state dictionary\n",
        "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "\n",
        "print(f\"Model state dictionary saved to: {MODEL_SAVE_PATH}\")"
      ],
      "metadata": {
        "id": "owOkGbdAG6yI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# : DOWNLOAD SAM CHECKPOINT ---\n",
        "\n",
        "print(\"Downloading the SAM (ViT-Base) model checkpoint (375MB)...\")\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
        "print(\"Download complete.\")"
      ],
      "metadata": {
        "id": "Gye102m5HBIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  INITIALIZE SAM (FOUNDATION MODEL) ---\n",
        "\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
        "\n",
        "# --- Configuration ---\n",
        "SAM_CHECKPOINT = \"sam_vit_b_01ec64.pth\"\n",
        "MODEL_TYPE = \"vit_b\"\n",
        "# ---------------------\n",
        "\n",
        "print(\"Loading SAM model... This may take a moment.\")\n",
        "# 1. Load the model architecture\n",
        "sam = sam_model_registry[MODEL_TYPE](checkpoint=SAM_CHECKPOINT)\n",
        "\n",
        "# 2. Move the model to the GPU\n",
        "sam.to(device=device)\n",
        "\n",
        "# 3. Initialize the AutomaticMaskGenerator\n",
        "# This is our \"zero-shot\" segmentation tool\n",
        "mask_generator = SamAutomaticMaskGenerator(sam)\n",
        "\n",
        "print(f\"SAM model '{MODEL_TYPE}' loaded and moved to {device}.\")\n",
        "print(\"Initialized SamAutomaticMaskGenerator for zero-shot inference.\")"
      ],
      "metadata": {
        "id": "ntJhi84VHGH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN ZERO-SHOT INFERENCE WITH SAM ---\n",
        "\n",
        "# Helper function to plot SAM's output\n",
        "def show_sam_anns(anns, ax):\n",
        "    if len(anns) == 0:\n",
        "        return\n",
        "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
        "    ax.set_autoscale_on(False)\n",
        "    for ann in sorted_anns:\n",
        "        m = ann['segmentation']\n",
        "        img = np.ones((m.shape[0], m.shape[1], 3))\n",
        "        color_mask = np.random.random((1, 3)).tolist()[0]\n",
        "        for i in range(3):\n",
        "            img[:,:,i] = color_mask[i]\n",
        "        ax.imshow(np.dstack((img, m*0.35))) # Show colored mask\n",
        "\n",
        "# --- Get an original image to test ---\n",
        "# Let's use the first image from our subset for a consistent test\n",
        "test_img_id = subset_img_ids[0] # From Cell 5\n",
        "img_info = coco.loadImgs([test_img_id])[0]\n",
        "img_path = os.path.join(imgDir, img_info['file_name'])\n",
        "\n",
        "print(f\"Loading original image: {img_info['file_name']}\")\n",
        "# SAM expects a standard OpenCV/NumPy image (H, W, C)\n",
        "image_bgr = cv2.imread(img_path)\n",
        "\n",
        "# --- FIX IS HERE ---\n",
        "# It's cv2.COLOR_BGR2RGB (with a 2), not cv2.COLOR_BGR_RGB\n",
        "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "# -------------------\n",
        "\n",
        "# --- Run SAM ---\n",
        "# This is the \"zero-shot\" inference step.\n",
        "print(\"Running SAM...\")\n",
        "sam_masks = mask_generator.generate(image_rgb)\n",
        "print(f\"SAM found {len(sam_masks)} potential objects/masks.\")\n",
        "\n",
        "# --- Plot SAM's output ---\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.imshow(image_rgb)\n",
        "show_sam_anns(sam_masks, plt.gca())\n",
        "plt.title(\"SAM Automatic Zero-Shot Segmentation\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kA6ghAZGHL_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# INITIALIZE SAM PREDICTOR ---\n",
        "\n",
        "from segment_anything import SamPredictor\n",
        "\n",
        "# We use the 'sam' model we already loaded onto the GPU\n",
        "predictor = SamPredictor(sam)\n",
        "\n",
        "print(\"SamPredictor initialized and ready.\")"
      ],
      "metadata": {
        "id": "PStaI1bQH-vQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SIDE-BY-SIDE COMPARISON (U-NET vs. SAM) ---\n",
        "\n",
        "# 1. --- SETUP: Choose ONE image to test ---\n",
        "# We'll use the first image from our subset (the kitchen scene)\n",
        "test_img_id = subset_img_ids[0]\n",
        "img_info = coco.loadImgs([test_img_id])[0]\n",
        "img_path = os.path.join(imgDir, img_info['file_name'])\n",
        "\n",
        "# 2. --- LOAD ORIGINAL IMAGE AND MASK ---\n",
        "# Load original image in two formats (PIL for U-Net, RGB for SAM/plotting)\n",
        "image_pil = Image.open(img_path).convert(\"RGB\")\n",
        "image_bgr = cv2.imread(img_path)\n",
        "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Load Ground Truth annotations\n",
        "ann_ids = coco.getAnnIds(imgIds=img_info['id'])\n",
        "anns = coco.loadAnns(ann_ids)\n",
        "\n",
        "# Create the full Ground Truth mask for plotting\n",
        "gt_mask_coco = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)\n",
        "for ann in anns:\n",
        "    gt_mask_coco = np.maximum(gt_mask_coco, coco.annToMask(ann))\n",
        "\n",
        "# 3. --- A) RUN U-NET PREDICTION (on this specific image) ---\n",
        "model.eval() # Set U-Net to eval mode\n",
        "\n",
        "# Get the transform function from CELL 7\n",
        "transform_fn = get_transforms()\n",
        "\n",
        "# Apply the same transforms U-Net was trained on\n",
        "# We pass a dummy mask_pil because the transform function expects two args\n",
        "mask_pil_dummy = Image.fromarray(np.zeros((img_info['height'], img_info['width']), dtype=np.uint8))\n",
        "image_tensor, _ = transform_fn(image_pil, mask_pil_dummy)\n",
        "\n",
        "# Add batch dimension [C, H, W] -> [1, C, H, W] and send to GPU\n",
        "image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Get U-Net's prediction\n",
        "    pred_logits_unet = model(image_tensor)\n",
        "    pred_mask_unet = (torch.sigmoid(pred_logits_unet) > 0.5).float()\n",
        "\n",
        "# Resize U-Net's prediction back to original size for comparison\n",
        "unet_pred_resized = TF.resize(\n",
        "    pred_mask_unet.cpu(),\n",
        "    (img_info['height'], img_info['width']),\n",
        "    interpolation=T.InterpolationMode.NEAREST\n",
        ").squeeze().numpy() # Squeeze to [H, W]\n",
        "\n",
        "# 4. --- B) RUN SAM PREDICTION (on this specific image) ---\n",
        "predictor.set_image(image_rgb) # Set image for SAM\n",
        "\n",
        "# Get COCO bounding boxes [x, y, w, h] as prompts\n",
        "input_boxes = [ann['bbox'] for ann in anns]\n",
        "sam_boxes = []\n",
        "for box in input_boxes:\n",
        "    x, y, w, h = box; sam_boxes.append([x, y, x + w, y + h])\n",
        "\n",
        "input_boxes_tensor = torch.tensor(sam_boxes, device=predictor.device)\n",
        "\n",
        "# Run SAM with box prompts\n",
        "sam_masks, _, _ = predictor.predict_torch(\n",
        "    point_coords=None, point_labels=None,\n",
        "    boxes=input_boxes_tensor,\n",
        "    multimask_output=False,\n",
        ")\n",
        "\n",
        "# Combine all of SAM's masks into one\n",
        "sam_combined_mask, _ = torch.max(sam_masks, dim=0)\n",
        "sam_combined_mask = sam_combined_mask.float()\n",
        "\n",
        "# 5. --- C) PLOT ALL FOUR ---\n",
        "fig, ax = plt.subplots(1, 4, figsize=(24, 8))\n",
        "\n",
        "ax[0].imshow(image_rgb)\n",
        "ax[0].set_title(\"Original Image\")\n",
        "ax[0].axis('off')\n",
        "\n",
        "ax[1].imshow(gt_mask_coco, cmap='gray')\n",
        "ax[1].set_title(\"Ground Truth Mask\")\n",
        "ax[1].axis('off')\n",
        "\n",
        "ax[2].imshow(unet_pred_resized, cmap='gray')\n",
        "ax[2].set_title(\"U-Net (Trained) Prediction\")\n",
        "ax[2].axis('off')\n",
        "\n",
        "ax[3].imshow(sam_combined_mask.cpu().squeeze().numpy(), cmap='gray')\n",
        "ax[3].set_title(\"SAM (Zero-Shot) Prediction\")\n",
        "ax[3].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Comparison cell complete (Corrected).\")"
      ],
      "metadata": {
        "id": "1EJLQ1AbIDnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELL 19 (Corrected): DEFINE ALL METRIC FUNCTIONS ---\n",
        "\n",
        "def calculate_metrics(preds, targets, threshold=0.5):\n",
        "    \"\"\"Calculates IoU, Precision, Recall, and F1 Score\"\"\"\n",
        "    # 1. Apply sigmoid and threshold\n",
        "    preds_binary = (torch.sigmoid(preds) > threshold).float()\n",
        "\n",
        "    # 2. Flatten tensors\n",
        "    preds_flat = preds_binary.view(-1).cpu()\n",
        "    targets_flat = targets.view(-1).cpu()\n",
        "\n",
        "    # 3. Calculate TP, FP, FN\n",
        "    tp = (preds_flat * targets_flat).sum()\n",
        "    fp = preds_flat.sum() - tp\n",
        "    fn = targets_flat.sum() - tp\n",
        "\n",
        "    # 4. Calculate Metrics (add 1e-6 to avoid division by zero)\n",
        "    iou = (tp + 1e-6) / (tp + fp + fn + 1e-6)\n",
        "    precision = (tp + 1e-6) / (tp + fp + 1e-6)\n",
        "    recall = (tp + 1e-6) / (tp + fn + 1e-6)\n",
        "\n",
        "    # --- THIS IS THE FIX ---\n",
        "    # Calculate F1 directly from TP, FP, FN. It's more stable.\n",
        "    f1 = (2 * tp + 1e-6) / (2 * tp + fp + fn + 1e-6)\n",
        "\n",
        "    return iou.item(), precision.item(), recall.item(), f1.item()\n",
        "\n",
        "print(\"Corrected metric functions (IoU, Precision, Recall, F1) defined.\")"
      ],
      "metadata": {
        "id": "Ahzup0sVJFQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  CREATE EVALUATION DATALOADER ---\n",
        "\n",
        "# We re-use the 'train_dataset' we already created\n",
        "eval_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=1, # Process one image at a time\n",
        "    shuffle=False, # Go in order\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "print(f\"Created 'eval_loader' with {len(eval_loader)} images (batch size 1).\")"
      ],
      "metadata": {
        "id": "K1TFhVMfJIEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# - FINAL EVALUATION LOOP (U-NET vs. SAM) ---\n",
        "\n",
        "# 1. Set models to evaluation mode\n",
        "model.eval() # U-Net\n",
        "predictor.set_image(np.zeros((1,1,3), dtype=np.uint8)) # Init SAM predictor\n",
        "\n",
        "# 2. Metric accumulators\n",
        "unet_metrics = {'iou': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "sam_metrics = {'iou': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "\n",
        "print(\"Starting final evaluation... This will take several minutes.\")\n",
        "\n",
        "# Use tqdm for a progress bar\n",
        "for i, (image_tensor, true_mask_tensor) in enumerate(tqdm(eval_loader)):\n",
        "\n",
        "    # --- A. GET U-NET PREDICTION ---\n",
        "    # Data is already on CPU from loader, move to GPU for model\n",
        "    image_tensor_gpu = image_tensor.to(device)\n",
        "    true_mask_gpu = true_mask_tensor.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        unet_logits = model(image_tensor_gpu)\n",
        "\n",
        "    # Calculate U-Net metrics against the 256x256 ground truth\n",
        "    unet_iou, unet_p, unet_r, unet_f1 = calculate_metrics(unet_logits.cpu(), true_mask_tensor)\n",
        "    unet_metrics['iou'].append(unet_iou)\n",
        "    unet_metrics['precision'].append(unet_p)\n",
        "    unet_metrics['recall'].append(unet_r)\n",
        "    unet_metrics['f1'].append(unet_f1)\n",
        "\n",
        "    # --- B. GET SAM PREDICTION ---\n",
        "\n",
        "    # 1. Get original image info\n",
        "    img_id = subset_img_ids[i] # Get the i-th ID since shuffle=False\n",
        "    img_info = coco.loadImgs([img_id])[0]\n",
        "    img_path = os.path.join(imgDir, img_info['file_name'])\n",
        "\n",
        "    # 2. Load original image for SAM\n",
        "    image_bgr = cv2.imread(img_path)\n",
        "    if image_bgr is None: continue # Skip if image is bad\n",
        "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # 3. Set SAM image\n",
        "    predictor.set_image(image_rgb)\n",
        "\n",
        "    # 4. Get box prompts\n",
        "    ann_ids = coco.getAnnIds(imgIds=img_info['id'])\n",
        "    anns = coco.loadAnns(ann_ids)\n",
        "    if not anns: continue # Skip if no annotations\n",
        "\n",
        "    input_boxes = [ann['bbox'] for ann in anns]\n",
        "    sam_boxes = []\n",
        "    for box in input_boxes:\n",
        "        x, y, w, h = box; sam_boxes.append([x, y, x + w, y + h])\n",
        "\n",
        "    input_boxes_tensor = torch.tensor(sam_boxes, device=predictor.device)\n",
        "\n",
        "    # 5. Run SAM predictor\n",
        "    with torch.no_grad():\n",
        "        sam_masks, _, _ = predictor.predict_torch(\n",
        "            point_coords=None, point_labels=None,\n",
        "            boxes=input_boxes_tensor,\n",
        "            multimask_output=False,\n",
        "        )\n",
        "\n",
        "    # 6. Combine SAM masks and resize to match ground truth tensor (256x256)\n",
        "    sam_combined_mask, _ = torch.max(sam_masks, dim=0)\n",
        "    sam_combined_mask = sam_combined_mask.unsqueeze(0) # [1, H, W]\n",
        "\n",
        "    # Resize!\n",
        "    sam_pred_resized = TF.resize(\n",
        "        sam_combined_mask.float(),\n",
        "        (IMG_SIZE, IMG_SIZE), # (256, 256)\n",
        "        interpolation=T.InterpolationMode.NEAREST\n",
        "    ).cpu() # [1, 256, 256]\n",
        "\n",
        "    # --- C. CALCULATE SAM METRICS ---\n",
        "    # We use sigmoid on SAM's output just to make the metric function happy\n",
        "    # (it expects logits, but SAM's output is 0/1, so sigmoid(10) is ~1)\n",
        "    sam_iou, sam_p, sam_r, sam_f1 = calculate_metrics(sam_pred_resized * 10, true_mask_tensor)\n",
        "    sam_metrics['iou'].append(sam_iou)\n",
        "    sam_metrics['precision'].append(sam_p)\n",
        "    sam_metrics['recall'].append(sam_r)\n",
        "    sam_metrics['f1'].append(sam_f1)\n",
        "\n",
        "print(\"--- Evaluation Complete ---\")"
      ],
      "metadata": {
        "id": "Mq4e4HviJMeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELL 22: PRINT FINAL RESULTS TABLE ---\n",
        "\n",
        "# Calculate averages\n",
        "avg_unet_iou = np.mean(unet_metrics['iou'])\n",
        "avg_unet_precision = np.mean(unet_metrics['precision'])\n",
        "avg_unet_recall = np.mean(unet_metrics['recall'])\n",
        "avg_unet_f1 = np.mean(unet_metrics['f1'])\n",
        "\n",
        "avg_sam_iou = np.mean(sam_metrics['iou'])\n",
        "avg_sam_precision = np.mean(sam_metrics['precision'])\n",
        "avg_sam_recall = np.mean(sam_metrics['recall'])\n",
        "avg_sam_f1 = np.mean(sam_metrics['f1'])\n",
        "\n",
        "# Print the table\n",
        "print(\"--- Comparative Results (Averaged over 1000 images) ---\")\n",
        "print(\"---------------------------------------------------------\")\n",
        "print(f\"| Metric      | U-Net (Trained) | SAM (Zero-Shot) |\")\n",
        "print(f\"|-------------|-----------------|-----------------|\")\n",
        "print(f\"| mIoU / Dice | {avg_unet_iou:<15.4f} | {avg_sam_iou:<15.4f} |\")\n",
        "print(f\"| Precision   | {avg_unet_precision:<15.4f} | {avg_sam_precision:<15.4f} |\")\n",
        "print(f\"| Recall      | {avg_unet_recall:<15.4f} | {avg_sam_recall:<15.4f} |\")\n",
        "print(f\"| F1-Score    | {avg_unet_f1:<15.4f} | {avg_sam_f1:<15.4f} |\")\n",
        "print(\"---------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "NYJ97eMgLLSg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}